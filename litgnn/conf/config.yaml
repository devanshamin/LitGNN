defaults:
  - _self_
  - dataset: ???
  - model: ???
  - scheduler@train.scheduler: noam
  - override hydra/job_logging: custom

dataset:
  save_dir: .cache
  pre_transform:
    _target_: litgnn.nn.models.cmpnn.featurization.FeaturesGenerator
    atom_messages: False

model:
  _target_: litgnn.nn.models.graph.GraphLevelGNN
  in_channels: ${train.dataset.num_node_features}
  out_channels: ${dataset.num_classes}
  edge_dim: ${train.dataset.num_edge_features}
  num_ffn_layers: 2
  pooling_func_name: global_mean_pool

train:
  num_seed_runs: 1  # No. of times to run the train-test split with different seeds
  seed: 1
  batch_size: 32

  dataset:
    split: scaffold_split
    split_sizes: [0.8, 0.1, 0.1]
    num_node_features: ???
    num_edge_features: ???

  trainer:
    _target_: pytorch_lightning.Trainer
    accelerator: gpu
    devices: auto
    max_epochs: 100
    log_every_n_steps: 10
    logger:
      _target_: pytorch_lightning.loggers.WandbLogger
      project: LitGNN
      group: ${model.model_cls} # i.e., CMPNN
      job_type: ${dataset.dataset_type}-${dataset.dataset_name} # i.e., molecule_net-bbbp
      name: Seed_${train.seed} # i.e., Seed_1, Seed_2
      dir: wandb
    callbacks:
      - _target_: pytorch_lightning.callbacks.RichProgressBar
      - _target_: pytorch_lightning.callbacks.ModelCheckpoint
        monitor: val_loss
        mode: min
        save_top_k: 1
        filename: '{epoch:02d}-{val_loss:.4f}'
      - _target_: pytorch_lightning.callbacks.EarlyStopping
        monitor: val_loss
        mode: min
        patience: 10

  optimizer:
    _target_: torch.optim.Adam
    lr: ${train.scheduler.init_lr[0]}
    weight_decay: 0
